{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import everything we need\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier, BaggingClassifier, VotingClassifier, GradientBoostingClassifier, ExtraTreesClassifier\n",
    "from sklearn import svm\n",
    "from sklearn import model_selection\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.linear_model import SGDClassifier, LinearRegression\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.svm import SVC, NuSVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\n",
    "from xgboost import XGBClassifier\n",
    "from catboost import Pool, cv, CatBoostClassifier, CatBoostRegressor\n",
    "import lightgbm as lgb\n",
    "import random\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper functions\n",
    "\n",
    "# binary error definition\n",
    "def bin_classification_err(real_y, y):\n",
    "    len_data = y.size\n",
    "    num_diff = 0.0\n",
    "    for i in range(len_data):\n",
    "        if (y[i] != real_y[i]):\n",
    "            num_diff += 1.0\n",
    "    return (num_diff / len_data)\n",
    "\n",
    "\n",
    "# Handle missing values in data\n",
    "def missing_values(original_data, method='omit', \n",
    "                   supply_data=[0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,\n",
    "                                15,16,17,18,19,20,21,22,23,24,25,26]):\n",
    "    \"\"\"\n",
    "    Replace missing values in original data according to given rules.\n",
    "    Parameters\n",
    "    ----------\n",
    "    original_data : numpy array\n",
    "        The data set containing NaN.\n",
    "    method : str, optional\n",
    "        'omit' : remove rows containing NaN. Default.\n",
    "        'mean' : replace NaN by the mean of its column.\n",
    "        'median' : replace NaN by the median of its column.\n",
    "        'zeros' : replace NaN by 0.\n",
    "        'change_and_add_flags' : replace NaN by the values specified in \n",
    "         supply_data at each corresponding columns. Then add new columns \n",
    "         with 0 = not NaN and 1 = is NaN.\n",
    "    supply_data : list of floats, optional\n",
    "        values to replace NaN in each column. The default is \n",
    "        [0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26].\n",
    "        'imputation' : fill in missing values by simple machine learning\n",
    "    Returns\n",
    "    -------\n",
    "    new_data : numpy array of size (row_original_data, \n",
    "               column_original_data + n_column_containing_missing)\n",
    "        The processed data array.\n",
    "    \"\"\"\n",
    "    if method == 'omit':\n",
    "        new_data = original_data[~np.isnan(original_data).any(axis=1)]\n",
    "        \n",
    "    elif method == 'mean':\n",
    "        non_nan_data = original_data[~np.isnan(original_data).any(axis=1)]\n",
    "        mean_row = np.mean(non_nan_data, axis=0)\n",
    "        for i_column in range(len(mean_row)):\n",
    "            original_data[:,i_column] = np.nan_to_num(original_data[:,i_column], \n",
    "                                                      nan=mean_row[i_column])\n",
    "            new_data = original_data\n",
    "            \n",
    "    elif method == 'median':\n",
    "        non_nan_data = original_data[~np.isnan(original_data).any(axis=1)]\n",
    "        median_row = np.median(non_nan_data, axis=0)\n",
    "        for i_column in range(len(median_row)):\n",
    "            original_data[:,i_column] = np.nan_to_num(original_data[:,i_column], \n",
    "                                                      nan=median_row[i_column])\n",
    "            new_data = original_data\n",
    "            \n",
    "    elif method == 'zeros':\n",
    "        new_data = np.nan_to_num(original_data, nan=0.0)\n",
    "        \n",
    "    elif method == 'change_and_add_flags':\n",
    "        import numpy.ma as ma\n",
    "        for i_column in range(27): # 27 columns in total, not including y\n",
    "            new_column = np.zeros(len(original_data[:,i_column]))\n",
    "            mask = np.ma.masked_invalid(original_data[:,i_column]).mask\n",
    "            new_column[mask] = 1\n",
    "            if np.sum(new_column) != 0:\n",
    "                new_column = np.expand_dims(new_column, axis=0)\n",
    "                new_column = new_column.transpose()\n",
    "                original_data = np.insert(original_data, [-1], new_column, axis=1)\n",
    "                original_data[:,i_column] = np.nan_to_num(original_data[:,i_column], \n",
    "                                                      nan=supply_data[i_column])\n",
    "                new_data = original_data\n",
    "                \n",
    "    elif method == 'imputation':\n",
    "        # to do\n",
    "        pass\n",
    "    \n",
    "    else: \n",
    "        print('Invalid option for treating missing data.')\n",
    "    \n",
    "    return new_data\n",
    "\n",
    "# Transform data into something (hopefully) more useful\n",
    "def get_trans_data(data):\n",
    "    tdata = []\n",
    "    for i in range(len(data)):\n",
    "        pt = []\n",
    "        for idx in range(2, 6):\n",
    "            pt.append(data[i][idx])\n",
    "        for idx in range(16, len(data[i])):\n",
    "            pt.append(data[i][idx])\n",
    "        # add last price / med price\n",
    "        div_price = data[i][0] / data[i][1]\n",
    "        pt.append(div_price)\n",
    "        # add ask spread\n",
    "        ask_spread = data[i][11] - data[i][15]\n",
    "        pt.append(ask_spread)\n",
    "        # add ask fractions\n",
    "        for j in range(12, 16):\n",
    "            frac = data[i][j] / data[i][11]\n",
    "            pt.append(frac)\n",
    "        # add bid spread\n",
    "        bid_spread = data[i][6] - data[i][10]\n",
    "        pt.append(bid_spread)\n",
    "        # add bid fractions\n",
    "        for j in range(7, 11):\n",
    "            frac = data[i][j] / data[i][6]\n",
    "            pt.append(frac)\n",
    "        \n",
    "        tdata.append(pt)\n",
    "    return np.array(tdata)\n",
    "\n",
    "# Transform data into something (hopefully) more useful as a tensor\n",
    "def get_tensor_data(data, labels):\n",
    "    tdata = []\n",
    "    for i in range(len(data)):\n",
    "        pt = []\n",
    "        for idx in range(2, 3):#6):\n",
    "            pt.append(data[i][idx])\n",
    "        for idx in range(16, 17):#len(data[i])):\n",
    "            pt.append(data[i][idx])\n",
    "        # add last price / med price\n",
    "        div_price = data[i][0] / data[i][1]\n",
    "        pt.append(div_price)\n",
    "        # add ask spread\n",
    "        ask_spread = data[i][11] - data[i][15]\n",
    "        pt.append(ask_spread)\n",
    "        # add bid spread\n",
    "        bid_spread = data[i][6] - data[i][10]\n",
    "        pt.append(bid_spread)\n",
    "        pt.append(labels[i])\n",
    "        tdata.append(pt)\n",
    "    return torch.tensor(tdata)\n",
    "\n",
    "def restrain_out(out):\n",
    "    for i in range(len(out)):\n",
    "        if (out[i] > 1.0):\n",
    "            out[i] = 1.0\n",
    "        elif (out[i] < 0.0):\n",
    "            out[i] = 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 1.          6.          1.00002603 -2.          1.00031235  1.00036441\n",
      "  1.00046853  1.00052059  0.8         0.99994794  0.99989588  0.99984382\n",
      "  0.99979175]\n"
     ]
    }
   ],
   "source": [
    "# Get data\n",
    "\n",
    "# Parse from csv files\n",
    "test = pd.read_csv(\"data/test.csv\")\n",
    "df_train = pd.read_csv('data/train.csv', index_col=0)\n",
    "df_test = pd.read_csv('data/test.csv', index_col=0)\n",
    "\n",
    "# Convert data and deal with missing values\n",
    "dtrain = df_train.values[1:]\n",
    "dtest = df_test.values[:]\n",
    "dtest = missing_values(dtest, method = \"zeros\")\n",
    "dtrain = missing_values(dtrain, method = \"zeros\")\n",
    "\n",
    "# Separate validation and training data\n",
    "X_all, Y_all = dtrain[:, :-1], dtrain[:, -1]\n",
    "X_val = X_all[0:50000]\n",
    "X_train = X_all[50000:(len(X_all) - 1)]\n",
    "Y_val = Y_all[0:50000]\n",
    "Y_train = Y_all[50000:(len(Y_all) - 1)]\n",
    "ran_train = list(zip(X_train, Y_train))\n",
    "random.shuffle(ran_train)\n",
    "X_train, Y_train = zip(*ran_train)\n",
    "X_test = dtest\n",
    "\n",
    "X_train_t = get_trans_data(X_train)\n",
    "X_val_t = get_trans_data(X_val)\n",
    "X_test_t = get_trans_data(X_test)\n",
    "X_all_t = get_trans_data(X_all)\n",
    "print(X_all_t[1000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Functions to make and train classifiers\n",
    "\n",
    "# Make and train a RandomForestClassifier\n",
    "def get_forest(n_estimators, depth, X, Y):\n",
    "    clf = RandomForestClassifier(n_estimators = n_estimators, max_depth = depth, criterion = 'gini')\n",
    "    clf.fit(X, Y)\n",
    "    return clf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rf1 validation error: 0.6356276897222657\n",
      "rf2 validation error: 0.6352894756179772\n",
      "gbc validation error: 0.6380958709934388\n",
      "xgb validation error: 0.6377406386216694\n",
      "eclf validation error: 0.6379139949158191\n"
     ]
    }
   ],
   "source": [
    "# Try out Voting Classifier\n",
    "\n",
    "train_size = 200000\n",
    "\n",
    "X = X_train_t[:train_size]\n",
    "Y = Y_train[:train_size]\n",
    "random_state = 54321\n",
    "\n",
    "test_trains = True\n",
    "\n",
    "clfs = []\n",
    "wts = []\n",
    "\n",
    "rf1 = RandomForestClassifier(n_estimators=500, criterion='entropy',  n_jobs = -1,  random_state=random_state, max_depth=8)\n",
    "if test_trains:\n",
    "    rf1.fit(X, Y)\n",
    "    # Get validation error\n",
    "    val_probs = rf1.predict_proba(X_val_t)[:,1]\n",
    "    val_err = roc_auc_score(Y_val, val_probs)\n",
    "    print(\"rf1 validation error:\", val_err)\n",
    "#clfs.append(('rf1', rf1))\n",
    "#wts.append(1)\n",
    "\n",
    "rf2 = RandomForestClassifier(n_estimators=500, criterion='gini',  n_jobs = -1, random_state=random_state, max_depth=8)\n",
    "if test_trains:\n",
    "    rf2.fit(X, Y)\n",
    "    val_probs = rf2.predict_proba(X_val_t)[:,1]\n",
    "    val_err = roc_auc_score(Y_val, val_probs)\n",
    "    print(\"rf2 validation error:\", val_err)\n",
    "clfs.append(('rf2', rf2))\n",
    "wts.append(1)\n",
    "\n",
    "gbc = GradientBoostingClassifier(random_state=random_state)\n",
    "if test_trains:\n",
    "    gbc.fit(X, Y)\n",
    "    val_probs = gbc.predict_proba(X_val_t)[:,1]\n",
    "    val_err = roc_auc_score(Y_val, val_probs)\n",
    "    print(\"gbc validation error:\", val_err)\n",
    "clfs.append(('gbc', gbc))\n",
    "wts.append(1)\n",
    "\n",
    "xgb = XGBClassifier(seed=random_state)\n",
    "if test_trains:\n",
    "    xgb.fit(X, Y)\n",
    "    val_probs = xgb.predict_proba(X_val_t)[:,1]\n",
    "    val_err = roc_auc_score(Y_val, val_probs)\n",
    "    print(\"xgb validation error:\", val_err)\n",
    "clfs.append(('xgb',xgb))\n",
    "wts.append(1)\n",
    "\n",
    "XGBClassifier(random_state=random_state)\n",
    "\n",
    "\n",
    "eclf = VotingClassifier(estimators=clfs, voting='soft')\n",
    "eclf.fit(X, Y)\n",
    "val_probs = eclf.predict_proba(X_val_t)[:,1]\n",
    "val_err = roc_auc_score(Y_val, val_probs)\n",
    "print(\"eclf validation error:\", val_err)\n",
    "\n",
    "# Make submission\n",
    "#test_probs = eclf.predict_proba(X_test_t)[:,1]\n",
    "#test[\"Predicted\"] = test_probs\n",
    "#test[[\"id\",\"Predicted\"]].to_csv(\"submission.csv\",index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done training model 1 (Gradient Boosting Classifier)\n",
      "Done training model 2 (XGB Classifier)\n",
      "Done training model 3 (Random Forest)\n",
      "Done training model 4 (AdaBoost)\n",
      "Done training model 5 (LinearDiscriminantAnalysis)\n",
      "Done training model 6 (DecisionTreeClassifier)\n",
      "Done training model 7 (Random Forest Entropy)\n",
      "Done training model 8 (QuadraticDiscriminantAnalysis)\n",
      "Model 1 (Gradient Boosting Classifier) validation error: 0.6527557871173288\n",
      "Model 2 (XGB Classifier) validation error: 0.6513079483623094\n",
      "Model 3 (Random Forest) validation error: 0.6510053316740823\n",
      "Model 4 (AdaBoost) validation error: 0.6471550642904701\n",
      "Model 5 (LinearDiscriminantAnalysis) validation error: 0.63172621598906\n",
      "Model 6 (DecisionTreeClassifier) validation error: 0.6424465068171215\n",
      "Model 7 (Random Forest Entropy) validation error: 0.6376918180463393\n",
      "Model 8 (QuadraticDiscriminantAnalysis) validation error: 0.6527321558594005\n",
      "[0.50088651 0.42309182 0.55757966 ... 0.59930218 0.23379786 0.35182111]\n",
      "Validation error: 0.6493157711544709\n"
     ]
    }
   ],
   "source": [
    "X = X_train_t[:200000]\n",
    "Y = Y_train[:200000]\n",
    "XV = X_train_t[200001:400000]\n",
    "YV = Y_train[200001:400000]\n",
    "\n",
    "random_state = 54321\n",
    "\n",
    "\n",
    "model1 = GradientBoostingClassifier(random_state=random_state)\n",
    "model2 = XGBClassifier(seed=random_state)\n",
    "model3 = RandomForestClassifier(n_estimators=400, criterion='gini',  n_jobs = -1, random_state=random_state, max_depth=8)\n",
    "model4 = AdaBoostClassifier(n_estimators=100)\n",
    "model5 = LinearDiscriminantAnalysis()\n",
    "model6 = DecisionTreeClassifier(max_depth=8)\n",
    "model7 = RandomForestClassifier(n_estimators=250, criterion='entropy',  n_jobs = -1,  random_state=random_state)\n",
    "model8 = GradientBoostingClassifier()\n",
    "\n",
    "model1.fit(X, Y)\n",
    "print(\"Done training model 1 (Gradient Boosting Classifier)\")\n",
    "model2.fit(X, Y)\n",
    "print(\"Done training model 2 (XGB Classifier)\")\n",
    "model3.fit(X, Y)\n",
    "print(\"Done training model 3 (Random Forest)\")\n",
    "model4.fit(X, Y)\n",
    "print(\"Done training model 4 (AdaBoost)\")\n",
    "model5.fit(X, Y)\n",
    "print(\"Done training model 5 (LinearDiscriminantAnalysis)\")\n",
    "model6.fit(X, Y)\n",
    "print(\"Done training model 6 (DecisionTreeClassifier)\")\n",
    "model7.fit(X, Y)\n",
    "print(\"Done training model 7 (Random Forest Entropy)\")\n",
    "model8.fit(X, Y)\n",
    "print(\"Done training model 8 (QuadraticDiscriminantAnalysis)\")\n",
    "\n",
    "\n",
    "preds1 = model1.predict_proba(XV)[:,1]\n",
    "preds2 = model2.predict_proba(XV)[:,1]\n",
    "preds3 = model3.predict_proba(XV)[:,1]\n",
    "preds4 = model4.predict_proba(XV)[:,1]\n",
    "preds5 = model5.predict_proba(XV)[:,1]\n",
    "preds6 = model6.predict_proba(XV)[:,1]\n",
    "preds7 = model7.predict_proba(XV)[:,1]\n",
    "preds8 = model8.predict_proba(XV)[:,1]\n",
    "\n",
    "tpreds1 = model1.predict_proba(X_test_t)[:,1]\n",
    "tpreds2 = model2.predict_proba(X_test_t)[:,1]\n",
    "tpreds3 = model3.predict_proba(X_test_t)[:,1]\n",
    "tpreds4 = model4.predict_proba(X_test_t)[:,1]\n",
    "tpreds5 = model5.predict_proba(X_test_t)[:,1]\n",
    "tpreds6 = model6.predict_proba(X_test_t)[:,1]\n",
    "tpreds7 = model7.predict_proba(X_test_t)[:,1]\n",
    "tpreds8 = model8.predict_proba(X_test_t)[:,1]\n",
    "\n",
    "test_preds1 = model1.predict_proba(X_val_t)[:,1]\n",
    "val1_err = roc_auc_score(Y_val, test_preds1)\n",
    "print(\"Model 1 (Gradient Boosting Classifier) validation error:\", val1_err)\n",
    "test_preds2 = model2.predict_proba(X_val_t)[:,1]\n",
    "val2_err = roc_auc_score(Y_val, test_preds2)\n",
    "print(\"Model 2 (XGB Classifier) validation error:\", val2_err)\n",
    "test_preds3 = model3.predict_proba(X_val_t)[:,1]\n",
    "val3_err = roc_auc_score(Y_val, test_preds3)\n",
    "print(\"Model 3 (Random Forest) validation error:\", val3_err)\n",
    "test_preds4 = model4.predict_proba(X_val_t)[:,1]\n",
    "val4_err = roc_auc_score(Y_val, test_preds4)\n",
    "print(\"Model 4 (AdaBoost) validation error:\", val4_err)\n",
    "test_preds5 = model5.predict_proba(X_val_t)[:,1]\n",
    "val5_err = roc_auc_score(Y_val, test_preds5)\n",
    "print(\"Model 5 (LinearDiscriminantAnalysis) validation error:\", val5_err)\n",
    "test_preds6 = model6.predict_proba(X_val_t)[:,1]\n",
    "val6_err = roc_auc_score(Y_val, test_preds6)\n",
    "print(\"Model 6 (DecisionTreeClassifier) validation error:\", val6_err)\n",
    "test_preds7 = model7.predict_proba(X_val_t)[:,1]\n",
    "val7_err = roc_auc_score(Y_val, test_preds7)\n",
    "print(\"Model 7 (Random Forest Entropy) validation error:\", val7_err)\n",
    "test_preds8 = model8.predict_proba(X_val_t)[:,1]\n",
    "val8_err = roc_auc_score(Y_val, test_preds8)\n",
    "print(\"Model 8 (QuadraticDiscriminantAnalysis) validation error:\", val8_err)\n",
    "\n",
    "stacked_predictions = np.column_stack((preds1, preds2, preds3, preds4, preds5, preds6, preds7, preds8))\n",
    "tstacked_predictions = np.column_stack((tpreds1, tpreds2, tpreds3, tpreds4, tpreds5, tpreds6, tpreds7, tpreds8))\n",
    "stacked_test_predictions = np.column_stack((test_preds1, test_preds2, test_preds3, test_preds4,\n",
    "                                            test_preds5, test_preds6, test_preds7, test_preds8))\n",
    "meta_model = LinearRegression()\n",
    "meta_model.fit(stacked_predictions, YV)\n",
    "final_predictions = meta_model.predict(stacked_test_predictions)\n",
    "restrain_out(final_predictions)\n",
    "print(final_predictions)\n",
    "val_err = roc_auc_score(Y_val, final_predictions)\n",
    "print(\"Validation error:\", val_err)\n",
    "# Make submission\n",
    "test_probs = meta_model.predict(tstacked_predictions)\n",
    "restrain_out(test_probs)\n",
    "test[\"Predicted\"] = test_probs\n",
    "test[[\"id\",\"Predicted\"]].to_csv(\"submission.csv\",index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "clf validation error: 0.6385600713957855\n"
     ]
    }
   ],
   "source": [
    "train_size = 500000\n",
    "\n",
    "X = X_train_t[:train_size]\n",
    "Y = Y_train[:train_size]\n",
    "random_state = 54321\n",
    "\n",
    "clf = XGBClassifier(random_state=random_state)\n",
    "clf.fit(X, Y)\n",
    "val_probs = clf.predict_proba(X_val_t)[:,1]\n",
    "val_err = roc_auc_score(Y_val, val_probs)\n",
    "print(\"clf validation error:\", val_err)\n",
    "# Make submission\n",
    "#test_probs = clf.predict_proba(X_test_t)[:, 1]\n",
    "#test[\"Predicted\"] = test_probs\n",
    "#test[[\"id\",\"Predicted\"]].to_csv(\"submission.csv\",index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gbc validation error: 0.6103300372827178\n"
     ]
    }
   ],
   "source": [
    "# Best model so far\n",
    "\n",
    "# Validation score: 0.639053452056516\n",
    "\n",
    "train_size = 500000\n",
    "\n",
    "X = X_train_t[:train_size]\n",
    "Y = Y_train[:train_size]\n",
    "random_state = 54321\n",
    "\n",
    "gbc = GradientBoostingClassifier(random_state=random_state)\n",
    "gbc.fit(X, Y)\n",
    "val_probs = gbc.predict_proba(X_val_t)[:,1]\n",
    "val_err = roc_auc_score(Y_val, val_probs)\n",
    "print(\"gbc validation error:\", val_err)\n",
    "# Make submission\n",
    "test_probs = gbc.predict_proba(X_test_t)[:, 1]\n",
    "test[\"Predicted\"] = test_probs\n",
    "test[[\"id\",\"Predicted\"]].to_csv(\"submission.csv\",index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done training model 1\n",
      "Done training model 2\n",
      "Done training model 3\n",
      "Model1 validation error: 0.6380958709934388\n",
      "Model2 validation error: 0.6377406386216694\n",
      "Model3 validation error: 0.6377406386216694\n",
      "[0.45821113 0.38814805 0.61930441 ... 0.59136322 0.36506096 0.41681025]\n",
      "Validation error: 0.6379781914842635\n"
     ]
    }
   ],
   "source": [
    "# validation error: 0.6467546999617307 0.0.6535391961510051 0.653722038522001\n",
    "\n",
    "X = X_train_t[:200000]\n",
    "Y = Y_train[:200000]\n",
    "XV = X_train_t[200001:400000]\n",
    "YV = Y_train[200001:400000]\n",
    "\n",
    "random_state = 54321\n",
    "\n",
    "model1 = GradientBoostingClassifier(random_state=random_state)\n",
    "model2 = XGBClassifier(seed=54321)\n",
    "model3 = RandomForestClassifier(n_estimators=100, criterion='gini',  n_jobs = -1, random_state=random_state, max_depth=8)\n",
    "\n",
    "model1.fit(X, Y)\n",
    "print(\"Done training model 1\")\n",
    "model2.fit(X, Y)\n",
    "print(\"Done training model 2\")\n",
    "model3.fit(X, Y)\n",
    "print(\"Done training model 3\")\n",
    "\n",
    "preds1 = model1.predict_proba(XV)[:,1]\n",
    "preds2 = model2.predict_proba(XV)[:,1]\n",
    "preds3 = model3.predict_proba(XV)[:,1]\n",
    "\n",
    "test_preds1 = model1.predict_proba(X_val_t)[:,1]\n",
    "val1_err = roc_auc_score(Y_val, test_preds1)\n",
    "print(\"Model1 validation error:\", val1_err)\n",
    "test_preds2 = model2.predict_proba(X_val_t)[:,1]\n",
    "val2_err = roc_auc_score(Y_val, test_preds2)\n",
    "print(\"Model2 validation error:\", val2_err)\n",
    "test_preds3 = model3.predict_proba(X_val_t)[:,1]\n",
    "val3_err = roc_auc_score(Y_val, test_preds2)\n",
    "print(\"Model3 validation error:\", val3_err)\n",
    "\n",
    "stacked_predictions = np.column_stack((preds1, preds2, preds3))\n",
    "stacked_test_predictions = np.column_stack((test_preds1, test_preds2, test_preds3))\n",
    "\n",
    "meta_model = LinearRegression()\n",
    "meta_model.fit(stacked_predictions, YV)\n",
    "final_predictions = meta_model.predict(stacked_test_predictions)\n",
    "print(final_predictions)\n",
    "val_err = roc_auc_score(Y_val, final_predictions)\n",
    "print(\"Validation error:\", val_err)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done training model 1\n",
      "Done training model 2\n",
      "Done training model 3\n"
     ]
    }
   ],
   "source": [
    "# validation error: 0.6467546999617307\n",
    "\n",
    "X = X_all_t[:250000]\n",
    "Y = Y_all[:250000]\n",
    "XV = X_all_t[250001:500000]\n",
    "YV = Y_all[250001:500000]\n",
    "\n",
    "random_state = 54321\n",
    "\n",
    "model1 = GradientBoostingClassifier(random_state=random_state)\n",
    "model2 = XGBClassifier(seed=random_state)\n",
    "model3 = RandomForestClassifier(n_estimators=1000, criterion='gini',  n_jobs = -1, random_state=random_state, max_depth=8)\n",
    "\n",
    "model1.fit(X, Y)\n",
    "print(\"Done training model 1\")\n",
    "model2.fit(X, Y)\n",
    "print(\"Done training model 2\")\n",
    "model3.fit(X, Y)\n",
    "print(\"Done training model 3\")\n",
    "\n",
    "preds1 = model1.predict_proba(XV)[:,1]\n",
    "preds2 = model2.predict_proba(XV)[:,1]\n",
    "preds3 = model3.predict_proba(XV)[:,1]\n",
    "\n",
    "test_preds1 = model1.predict_proba(X_test_t)[:,1]\n",
    "#val1_err = roc_auc_score(Y_val, test_preds1)\n",
    "#print(\"Model1 validation error:\", val1_err)\n",
    "test_preds2 = model2.predict_proba(X_test_t)[:,1]\n",
    "#val2_err = roc_auc_score(Y_val, test_preds2)\n",
    "#print(\"Model2 validation error:\", val2_err)\n",
    "test_preds3 = model3.predict_proba(X_test_t)[:,1]\n",
    "#val3_err = roc_auc_score(Y_val, test_preds2)\n",
    "#print(\"Model3 validation error:\", val3_err)\n",
    "\n",
    "stacked_predictions = np.column_stack((preds1, preds2, preds3))\n",
    "stacked_test_predictions = np.column_stack((test_preds1, test_preds2, test_preds3))\n",
    "\n",
    "meta_model = LinearRegression()\n",
    "meta_model.fit(stacked_predictions, YV)\n",
    "final_predictions = meta_model.predict(stacked_test_predictions)\n",
    "restrain_out(final_predictions)\n",
    "test[\"Predicted\"] = final_predictions\n",
    "test[[\"id\",\"Predicted\"]].to_csv(\"submission.csv\",index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Assertion `x >= 0. && x <= 1.' failed. input value should be between 0~1, but got -0.139637 at /Users/distiller/project/conda/conda-bld/pytorch_1579022061893/work/aten/src/THNN/generic/BCECriterion.c:60",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-58-bad5964aea22>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m         \u001b[0;31m# Calculate loss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m         \u001b[0;31m# Backward pass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    530\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    531\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 532\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    533\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    534\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input, target)\u001b[0m\n\u001b[1;32m    496\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    497\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 498\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbinary_cross_entropy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreduction\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreduction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    499\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    500\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/site-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mbinary_cross_entropy\u001b[0;34m(input, target, weight, size_average, reduce, reduction)\u001b[0m\n\u001b[1;32m   2075\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2076\u001b[0m     return torch._C._nn.binary_cross_entropy(\n\u001b[0;32m-> 2077\u001b[0;31m         input, target, weight, reduction_enum)\n\u001b[0m\u001b[1;32m   2078\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2079\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Assertion `x >= 0. && x <= 1.' failed. input value should be between 0~1, but got -0.139637 at /Users/distiller/project/conda/conda-bld/pytorch_1579022061893/work/aten/src/THNN/generic/BCECriterion.c:60"
     ]
    }
   ],
   "source": [
    "data = get_tensor_data(X_train, Y_train)\n",
    "X_val_ten = torch.tensor(X_train_t)\n",
    "\n",
    "dropout = 0.5\n",
    "model = nn.Sequential(\n",
    "    nn.Linear(17, 10),\n",
    "    nn.Softmax(),\n",
    "    nn.Dropout(dropout),\n",
    "    nn.Linear(10, 5),\n",
    "    nn.Softmax(),\n",
    "    nn.Dropout(dropout),\n",
    "    nn.Linear(5, 3),\n",
    "    nn.Softmax(),\n",
    "    nn.Dropout(dropout),\n",
    "    nn.Linear(3, 1)\n",
    ")\n",
    "\n",
    "num_epochs = 5\n",
    "batch_size = 128\n",
    "step = len(data) / batch_size - 1\n",
    "loss_fn = nn.BCELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(get_tensor_data(X_train, Y_train), batch_size=batch_size, shuffle=True)\n",
    "test_loader = torch.utils.data.DataLoader(get_tensor_data(X_val, Y_val), batch_size=batch_size, shuffle=True) \n",
    "\n",
    "model.train()\n",
    "\n",
    "# Some layers, such as Dropout, behave differently during training\n",
    "model.train()\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    b = 0\n",
    "    while b < len(data):\n",
    "        batch = data[b:(b + batch_size), :-1]\n",
    "        target = data[b:(b + batch_size), -1]\n",
    "        # Erase accumulated gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Forward pass\n",
    "        output = model(batch)\n",
    "\n",
    "        # Calculate loss\n",
    "        loss = loss_fn(output, target)\n",
    "\n",
    "        # Backward pass\n",
    "        loss.backward()\n",
    "        \n",
    "        # Weight update\n",
    "        optimizer.step()\n",
    "        \n",
    "        b += batch_size\n",
    "\n",
    "    # Track loss each epoch\n",
    "    print('Train Epoch: %d  Loss: %.4f' % (epoch + 1,  loss.item()))\n",
    "    \n",
    "    \n",
    "model.eval()\n",
    "\n",
    "# Turning off automatic differentiation\n",
    "with torch.no_grad():\n",
    "    val_pred = model(X_val_ten.float())\n",
    "    print(val_pred)\n",
    "\n",
    "val_err = roc_auc_score(Y_val, val_pred)\n",
    "print(\"Validation error:\", val_err)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done!\n"
     ]
    }
   ],
   "source": [
    "# Best model so far\n",
    "\n",
    "# Validation score: 0.639053452056516\n",
    "\n",
    "train_size = 500000\n",
    "\n",
    "X = X_all_t\n",
    "Y = Y_all\n",
    "random_state = 54321\n",
    "\n",
    "gbc = GradientBoostingClassifier(random_state=random_state)\n",
    "gbc.fit(X, Y)\n",
    "#val_probs = gbc.predict_proba(X_val_t)[:,1]\n",
    "#val_err = roc_auc_score(Y_val, val_probs)\n",
    "#print(\"gbc validation error:\", val_err)\n",
    "# Make submission\n",
    "test_probs = gbc.predict_proba(X_test_t)[:, 1]\n",
    "test[\"Predicted\"] = test_probs\n",
    "test[[\"id\",\"Predicted\"]].to_csv(\"submission.csv\",index=False)\n",
    "print(\"done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
