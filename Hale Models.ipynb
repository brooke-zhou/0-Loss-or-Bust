{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import everything we need\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\n",
    "from sklearn import svm\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import roc_auc_score\n",
    "import random\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "np.random.seed(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Classification error definition\n",
    "def classification_err(y, real_y):\n",
    "    len_data = y.size\n",
    "    num_diff = 0.0\n",
    "    for i in range(len_data):\n",
    "        if (y[i] != real_y[i]):\n",
    "            num_diff += 1.0\n",
    "    return (num_diff / len_data)\n",
    "\n",
    "def missing_values(original_data, method='omit', \n",
    "                   supply_data=[0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,\n",
    "                                15,16,17,18,19,20,21,22,23,24,25,26]):\n",
    "    \"\"\"\n",
    "    Replace missing values in original data according to given rules.\n",
    "    Parameters\n",
    "    ----------\n",
    "    original_data : numpy array\n",
    "        The data set containing NaN.\n",
    "    method : str, optional\n",
    "        'omit' : remove rows containing NaN. Default.\n",
    "        'mean' : replace NaN by the mean of its column.\n",
    "        'median' : replace NaN by the median of its column.\n",
    "        'zeros' : replace NaN by 0.\n",
    "        'change_and_add_flags' : replace NaN by the values specified in \n",
    "         supply_data at each corresponding columns. Then add new columns \n",
    "         with 0 = not NaN and 1 = is NaN.\n",
    "    supply_data : list of floats, optional\n",
    "        values to replace NaN in each column. The default is \n",
    "        [0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26].\n",
    "        'imputation' : fill in missing values by simple machine learning\n",
    "    Returns\n",
    "    -------\n",
    "    new_data : numpy array of size (row_original_data, \n",
    "               column_original_data + n_column_containing_missing)\n",
    "        The processed data array.\n",
    "    \"\"\"\n",
    "    if method == 'omit':\n",
    "        new_data = original_data[~np.isnan(original_data).any(axis=1)]\n",
    "        \n",
    "    elif method == 'mean':\n",
    "        non_nan_data = original_data[~np.isnan(original_data).any(axis=1)]\n",
    "        mean_row = np.mean(non_nan_data, axis=0)\n",
    "        for i_column in range(len(mean_row)):\n",
    "            original_data[:,i_column] = np.nan_to_num(original_data[:,i_column], \n",
    "                                                      nan=mean_row[i_column])\n",
    "            new_data = original_data\n",
    "            \n",
    "    elif method == 'median':\n",
    "        non_nan_data = original_data[~np.isnan(original_data).any(axis=1)]\n",
    "        median_row = np.median(non_nan_data, axis=0)\n",
    "        for i_column in range(len(median_row)):\n",
    "            original_data[:,i_column] = np.nan_to_num(original_data[:,i_column], \n",
    "                                                      nan=median_row[i_column])\n",
    "            new_data = original_data\n",
    "            \n",
    "    elif method == 'zeros':\n",
    "        new_data = np.nan_to_num(original_data, nan=0.0)\n",
    "        \n",
    "    elif method == 'change_and_add_flags':\n",
    "        import numpy.ma as ma\n",
    "        for i_column in range(27): # 27 columns in total, not including y\n",
    "            new_column = np.zeros(len(original_data[:,i_column]))\n",
    "            mask = np.ma.masked_invalid(original_data[:,i_column]).mask\n",
    "            new_column[mask] = 1\n",
    "            if np.sum(new_column) != 0:\n",
    "                new_column = np.expand_dims(new_column, axis=0)\n",
    "                new_column = new_column.transpose()\n",
    "                original_data = np.insert(original_data, [-1], new_column, axis=1)\n",
    "                original_data[:,i_column] = np.nan_to_num(original_data[:,i_column], \n",
    "                                                      nan=supply_data[i_column])\n",
    "                new_data = original_data\n",
    "                \n",
    "    elif method == 'imputation':\n",
    "        # to do\n",
    "        pass\n",
    "    \n",
    "    else: \n",
    "        print('Invalid option for treating missing data.')\n",
    "    \n",
    "    return new_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get data\n",
    "\n",
    "test = pd.read_csv(\"data/test.csv\")\n",
    "df_train = pd.read_csv('data/train.csv', index_col=0)\n",
    "df_test = pd.read_csv('data/test.csv', index_col=0)\n",
    "\n",
    "\n",
    "df_train = df_train.dropna()\n",
    "# don't need labels\n",
    "dtrain = df_train.values[1:]\n",
    "dtest = df_test.values[:]\n",
    "dtest = missing_values(dtest, method = \"median\")\n",
    "\n",
    "#random.shuffle(dtrain)\n",
    "\n",
    "X_all, Y_all = dtrain[:, :-1], dtrain[:, -1]\n",
    "\n",
    "X_val = X_all[0:10000]\n",
    "X_train = X_all[10000:100000]\n",
    "Y_val = Y_all[0:10000]\n",
    "Y_train = Y_all[10000:100000]\n",
    "\n",
    "X_test = dtest\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done training\n",
      "Validation error: 0.6057279432421964\n",
      "[0.4803708  0.34472494 0.4199356  ... 0.42304598 0.44360754 0.54446325]\n"
     ]
    }
   ],
   "source": [
    "# Try out random forest\n",
    "\n",
    "n_estimators = 1000\n",
    "depth = 15\n",
    "\n",
    "clf = RandomForestClassifier(n_estimators = n_estimators, max_depth = depth, criterion = 'gini')\n",
    "clf.fit(X_train, Y_train)\n",
    "print(\"Done training\")\n",
    "\n",
    "y_val_pred = clf.predict_proba(X_val)[:,1]\n",
    "#print(y_val_pred)\n",
    "val_err = roc_auc_score(Y_val, y_val_pred)\n",
    "print(\"Validation error:\", val_err)\n",
    "\n",
    "# Make submission\n",
    "test_probs = clf.predict_proba(X_test)[:, 1]\n",
    "print(test_probs)\n",
    "test[\"Predicted\"] = test_probs\n",
    "test[[\"id\",\"Predicted\"]].to_csv(\"submission.csv\",index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done training\n",
      "On train pt: 1\n",
      "I think it is: [0.]\n",
      "I think it is: [0.]\n",
      "I think it is: [0.]\n",
      "I think it is: [0.]\n",
      "I think it is: [0.]\n",
      "I think it is: [0.]\n",
      "I think it is: [0.]\n",
      "I think it is: [0.]\n",
      "I think it is: [0.]\n",
      "I think it is: [0.]\n",
      "On train pt: 2\n",
      "I think it is: [0.]\n",
      "I think it is: [0.]\n",
      "I think it is: [0.]\n",
      "I think it is: [0.]\n",
      "I think it is: [0.]\n",
      "I think it is: [0.]\n",
      "I think it is: [0.]\n",
      "I think it is: [0.]\n",
      "I think it is: [0.]\n",
      "I think it is: [0.]\n",
      "On train pt: 3\n",
      "I think it is: [0.]\n",
      "I think it is: [0.]\n",
      "I think it is: [0.]\n",
      "I think it is: [0.]\n",
      "I think it is: [0.]\n",
      "I think it is: [0.]\n",
      "I think it is: [0.]\n",
      "I think it is: [0.]\n",
      "I think it is: [0.]\n",
      "I think it is: [0.]\n",
      "On train pt: 4\n",
      "I think it is: [0.]\n",
      "I think it is: [0.]\n",
      "I think it is: [0.]\n",
      "I think it is: [0.]\n",
      "I think it is: [0.]\n",
      "I think it is: [0.]\n",
      "I think it is: [0.]\n",
      "I think it is: [0.]\n",
      "I think it is: [0.]\n",
      "I think it is: [0.]\n",
      "On train pt: 5\n",
      "I think it is: [0.]\n",
      "I think it is: [0.]\n",
      "I think it is: [0.]\n",
      "I think it is: [0.]\n",
      "I think it is: [0.]\n",
      "I think it is: [0.]\n",
      "I think it is: [0.]\n",
      "I think it is: [0.]\n",
      "I think it is: [0.]\n",
      "I think it is: [0.]\n",
      "On train pt: 6\n",
      "I think it is: [0.]\n",
      "I think it is: [0.]\n",
      "I think it is: [0.]\n",
      "I think it is: [0.]\n",
      "I think it is: [0.]\n",
      "I think it is: [0.]\n",
      "I think it is: [0.]\n",
      "I think it is: [0.]\n",
      "I think it is: [0.]\n",
      "I think it is: [0.]\n",
      "On train pt: 7\n",
      "I think it is: [0.]\n",
      "I think it is: [0.]\n",
      "I think it is: [0.]\n",
      "I think it is: [0.]\n",
      "I think it is: [0.]\n",
      "I think it is: [0.]\n",
      "I think it is: [0.]\n",
      "I think it is: [0.]\n",
      "I think it is: [0.]\n",
      "I think it is: [0.]\n",
      "On train pt: 8\n",
      "I think it is: [0.]\n",
      "I think it is: [0.]\n",
      "I think it is: [0.]\n",
      "I think it is: [0.]\n",
      "I think it is: [0.]\n",
      "I think it is: [0.]\n",
      "I think it is: [0.]\n",
      "I think it is: [0.]\n",
      "I think it is: [0.]\n",
      "I think it is: [0.]\n",
      "On train pt: 9\n",
      "I think it is: [0.]\n",
      "I think it is: [0.]\n",
      "I think it is: [0.]\n",
      "I think it is: [0.]\n",
      "I think it is: [0.]\n",
      "I think it is: [0.]\n",
      "I think it is: [0.]\n",
      "I think it is: [0.]\n",
      "I think it is: [0.]\n",
      "I think it is: [0.]\n",
      "On train pt: 10\n",
      "I think it is: [0.]\n",
      "I think it is: [0.]\n",
      "I think it is: [0.]\n",
      "I think it is: [0.]\n",
      "I think it is: [0.]\n",
      "I think it is: [0.]\n",
      "I think it is: [0.]\n",
      "I think it is: [0.]\n",
      "I think it is: [0.]\n",
      "I think it is: [0.]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.7/site-packages/sklearn/svm/base.py:193: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
      "  \"avoid this warning.\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0. 0. 0. ... 0. 0. 0.]\n",
      "[0. 0. 0. ... 0. 0. 0.]\n"
     ]
    }
   ],
   "source": [
    "# Try out SVM\n",
    "# Idea is to train SVM's on random datasets, then aggregate predictions to get a probability\n",
    "# Broken, not sure why. Going to try NN for now\n",
    "\n",
    "num_svms = 10\n",
    "set_size = 100\n",
    "\n",
    "def make_data(size):\n",
    "    X_set = []\n",
    "    Y_set = []\n",
    "    for i in range(size):\n",
    "        idx = random.randint(0, len(X_train) - 1)\n",
    "        X_set.append(X_train[idx])\n",
    "        Y_set.append(Y_train[idx])\n",
    "    return X_set, Y_set\n",
    "\n",
    "\n",
    "# Train svms      \n",
    "svms = []\n",
    "for i in range(num_svms):\n",
    "    #X_set, Y_set = make_data(set_size)\n",
    "    clf = svm.SVC(kernel='rbf', gamma='auto')\n",
    "    clf.fit(X_train, Y_train)\n",
    "    svms.append(clf)\n",
    "print(\"Done training\")\n",
    "    \n",
    "# Make test predictions\n",
    "test_probs = []\n",
    "for i in range(10):\n",
    "    print(\"On train pt:\", i + 1)\n",
    "    x = X_test[i]\n",
    "    prob = 0.0\n",
    "    for i in range(num_svms):\n",
    "        y = clf.predict([x])\n",
    "        print(\"I think it is:\", y)\n",
    "        prob += y[0]\n",
    "    prob = prob / num_svms\n",
    "    test_probs.append(prob)\n",
    "\n",
    "clf = svm.SVC(kernel='rbf', random_state=0)\n",
    "clf.fit(X_train, Y_train)\n",
    "\n",
    "test_probs = clf.predict(X_test)\n",
    "print(y_val_pred)\n",
    "#val_err = classification_err(y_val_pred, Y_val)\n",
    "#print(\"Validation error:\", val_err)\n",
    "\n",
    "# Make submission\n",
    "#test_probs = clf.predict_proba(X_test)[:, 1]\n",
    "print(test_probs)\n",
    "#test[\"Predicted\"] = test_probs\n",
    "#test[[\"id\",\"Predicted\"]].to_csv(\"submission.csv\",index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1  Loss: 8.9891\n",
      "Train Epoch: 2  Loss: 7.9952\n",
      "Train Epoch: 3  Loss: 7.2699\n",
      "Train Epoch: 4  Loss: 6.8461\n",
      "Train Epoch: 5  Loss: 6.3423\n",
      "Train Epoch: 6  Loss: 5.8945\n",
      "Train Epoch: 7  Loss: 5.4177\n",
      "Train Epoch: 8  Loss: 4.9366\n",
      "Train Epoch: 9  Loss: 4.4287\n",
      "Train Epoch: 10  Loss: 4.0057\n",
      "Train Epoch: 11  Loss: 3.6376\n",
      "Train Epoch: 12  Loss: 3.2474\n",
      "Train Epoch: 13  Loss: 2.9098\n",
      "Train Epoch: 14  Loss: 2.6031\n",
      "Train Epoch: 15  Loss: 2.3565\n",
      "Train Epoch: 16  Loss: 2.1069\n",
      "Train Epoch: 17  Loss: 1.8471\n",
      "Train Epoch: 18  Loss: 1.6933\n",
      "Train Epoch: 19  Loss: 1.5807\n",
      "Train Epoch: 20  Loss: 1.4561\n",
      "tensor([[1.2716, 0.7484],\n",
      "        [1.3235, 0.8651],\n",
      "        [1.3213, 0.8571],\n",
      "        ...,\n",
      "        [1.3668, 0.9806],\n",
      "        [1.3562, 0.9438],\n",
      "        [1.3680, 0.9792]])\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "bad input shape torch.Size([10000, 2])",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-39-ffce00c3a21f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     63\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval_probs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m     \u001b[0;31m#val_probs = torch.exp(val_probs)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 65\u001b[0;31m     \u001b[0mval_err\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mroc_auc_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mY_val\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_probs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     66\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Validation error:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_err\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/site-packages/sklearn/metrics/ranking.py\u001b[0m in \u001b[0;36mroc_auc_score\u001b[0;34m(y_true, y_score, average, sample_weight, max_fpr)\u001b[0m\n\u001b[1;32m    353\u001b[0m     return _average_binary_score(\n\u001b[1;32m    354\u001b[0m         \u001b[0m_binary_roc_auc_score\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_score\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maverage\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 355\u001b[0;31m         sample_weight=sample_weight)\n\u001b[0m\u001b[1;32m    356\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    357\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/site-packages/sklearn/metrics/base.py\u001b[0m in \u001b[0;36m_average_binary_score\u001b[0;34m(binary_metric, y_true, y_score, average, sample_weight)\u001b[0m\n\u001b[1;32m     74\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     75\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0my_type\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"binary\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 76\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mbinary_metric\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_score\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     77\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     78\u001b[0m     \u001b[0mcheck_consistent_length\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_score\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/site-packages/sklearn/metrics/ranking.py\u001b[0m in \u001b[0;36m_binary_roc_auc_score\u001b[0;34m(y_true, y_score, sample_weight)\u001b[0m\n\u001b[1;32m    325\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    326\u001b[0m         fpr, tpr, _ = roc_curve(y_true, y_score,\n\u001b[0;32m--> 327\u001b[0;31m                                 sample_weight=sample_weight)\n\u001b[0m\u001b[1;32m    328\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mmax_fpr\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mmax_fpr\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    329\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mauc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfpr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtpr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/site-packages/sklearn/metrics/ranking.py\u001b[0m in \u001b[0;36mroc_curve\u001b[0;34m(y_true, y_score, pos_label, sample_weight, drop_intermediate)\u001b[0m\n\u001b[1;32m    620\u001b[0m     \"\"\"\n\u001b[1;32m    621\u001b[0m     fps, tps, thresholds = _binary_clf_curve(\n\u001b[0;32m--> 622\u001b[0;31m         y_true, y_score, pos_label=pos_label, sample_weight=sample_weight)\n\u001b[0m\u001b[1;32m    623\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    624\u001b[0m     \u001b[0;31m# Attempt to drop thresholds corresponding to points in between and\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/site-packages/sklearn/metrics/ranking.py\u001b[0m in \u001b[0;36m_binary_clf_curve\u001b[0;34m(y_true, y_score, pos_label, sample_weight)\u001b[0m\n\u001b[1;32m    398\u001b[0m     \u001b[0mcheck_consistent_length\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_score\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    399\u001b[0m     \u001b[0my_true\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcolumn_or_1d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 400\u001b[0;31m     \u001b[0my_score\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcolumn_or_1d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_score\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    401\u001b[0m     \u001b[0massert_all_finite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    402\u001b[0m     \u001b[0massert_all_finite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_score\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mcolumn_or_1d\u001b[0;34m(y, warn)\u001b[0m\n\u001b[1;32m    758\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mravel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    759\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 760\u001b[0;31m     \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"bad input shape {0}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    761\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    762\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: bad input shape torch.Size([10000, 2])"
     ]
    }
   ],
   "source": [
    "# Trying out NN\n",
    "\n",
    "model = nn.Sequential(\n",
    "    nn.Linear(26, 20),\n",
    "    nn.ReLU(),\n",
    "    nn.Dropout(0.1),\n",
    "    nn.Linear(20, 15),\n",
    "    nn.ReLU(),\n",
    "    nn.Dropout(0.1),\n",
    "    nn.Linear(15, 10),\n",
    "    nn.ReLU(),\n",
    "    nn.Dropout(0.1),\n",
    "    nn.Linear(10, 5),\n",
    "    nn.ReLU(),\n",
    "    nn.Dropout(0.1),\n",
    "    nn.Linear(5, 2)\n",
    ")\n",
    "\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "\n",
    "# Train model\n",
    "model.train()\n",
    "\n",
    "num_epochs = 20\n",
    "X = X_train\n",
    "Y = Y_train\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    \n",
    "    data = torch.tensor(X).float()\n",
    "    target = torch.tensor(Y).long()\n",
    "    \n",
    "    # Erase accumulated gradients\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    # Forward pass\n",
    "    output = model(data)\n",
    "\n",
    "    # Calculate loss\n",
    "    loss = loss_fn(output, target)\n",
    "\n",
    "    # Backward pass\n",
    "    loss.backward()\n",
    "        \n",
    "    # Weight update\n",
    "    optimizer.step()\n",
    "\n",
    "    # Track loss each epoch\n",
    "    print('Train Epoch: %d  Loss: %.4f' % (epoch + 1,  loss.item()))\n",
    "    \n",
    "    \n",
    "# Putting layers like Dropout into evaluation mode\n",
    "model.eval()\n",
    "\n",
    "#test_probs = model(torch.tensor(X_test).float())[:,1]\n",
    "#print(test_probs)\n",
    "#print(torch.exp(test_probs))\n",
    "# Turning off automatic differentiation\n",
    "with torch.no_grad():\n",
    "    # Get validation error\n",
    "    val_probs = model(torch.tensor(X_val).float())\n",
    "    print(val_probs)\n",
    "    #val_probs = torch.exp(val_probs)\n",
    "    val_err = roc_auc_score(Y_val, val_probs)\n",
    "    print(\"Validation error:\", val_err)\n",
    "    \n",
    "    test_probs = model(torch.tensor(X_test).float())[:,1]\n",
    "    print(test_probs)\n",
    "    #test_probs = torch.exp(test_probs)\n",
    "    #print(test_probs)\n",
    "    test[\"Predicted\"] = test_probs\n",
    "    test[[\"id\",\"Predicted\"]].to_csv(\"submission.csv\",index=False)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done training\n",
      "Validation error: 0.5655748871256283\n",
      "[0.40086314 0.70657799 0.55767697 ... 0.55307357 0.59432705 0.45561421]\n"
     ]
    }
   ],
   "source": [
    "# Trying adaboost I guess?\n",
    "# Achieving VERY low classification error, but does not receive a very good score....\n",
    "\n",
    "clf = AdaBoostClassifier(\n",
    "    DecisionTreeClassifier(max_depth=7),\n",
    "    n_estimators=100\n",
    ")\n",
    "clf.fit(X_train, Y_train)\n",
    "print(\"Done training\")\n",
    "\n",
    "y_val_pred = clf.predict_proba(X_val)[:,1]\n",
    "val_err = roc_auc_score(Y_val, y_val_pred)\n",
    "print(\"Validation error:\", val_err)\n",
    "\n",
    "# Make submission\n",
    "test_probs = clf.predict_proba(X_test)[:,1]\n",
    "print(test_probs)\n",
    "test[\"Predicted\"] = test_probs\n",
    "test[[\"id\",\"Predicted\"]].to_csv(\"submission.csv\",index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
